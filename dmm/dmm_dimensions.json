{
  "level_labels": {
    "1": "Initial (Ad hoc)",
    "2": "Managed (Repeatable)",
    "3": "Defined",
    "4": "Quantitatively Managed",
    "5": "Optimized"
  },
  "peer_benchmark": {
    "governance": 3.2,
    "quality": 3.0,
    "metadata": 2.6,
    "architecture": 3.4,
    "security": 3.6,
    "analytics": 3.1
  },
  "dimensions": [
    {
      "key": "governance",
      "name": "Data Governance & Stewardship",
      "icon": "\ud83c\udfdb\ufe0f",
      "weight": 1.2,
      "description": "Ownership, policies, standards, and stewardship for enterprise data assets.",
      "next_steps": [
        "Establish a formal Data Governance Council and RACI for critical domains.",
        "Publish enterprise data policies and standards; enforce via change management.",
        "Stand up a stewardship workflow with issue intake, triage, and remediation SLAs."
      ],
      "sub_dimensions": [
        {
          "key": "ownership",
          "name": "Ownership & Roles",
          "levels": {
            "1": "No defined data owners or stewards; responsibilities unclear.",
            "2": "Some owners identified in key systems; informal responsibilities.",
            "3": "Owners and stewards defined across most domains; onboarding process exists.",
            "4": "Formal RACI; stewardship capacity planned; KPIs for policy adherence.",
            "5": "Ownership embedded in org design; continuous improvement & automation."
          }
        },
        {
          "key": "policy",
          "name": "Policy & Standards",
          "levels": {
            "1": "No enterprise data policies or standards.",
            "2": "Draft policies exist; adoption inconsistent.",
            "3": "Approved policies and standards exist; broad adoption.",
            "4": "Policies measured and enforced; periodic audits.",
            "5": "Policies embedded in SDLC/tooling; auto-conformance checks."
          }
        },
        {
          "key": "issue_mgmt",
          "name": "Issue Management",
          "levels": {
            "1": "Issues handled ad hoc; no central log.",
            "2": "Issues tracked in spreadsheets; limited ownership.",
            "3": "Central issue register with owners and due dates.",
            "4": "Root-cause analysis and trend reporting; SLA-based.",
            "5": "Proactive detection and auto-remediation for recurring issues."
          }
        }
      ]
    },
    {
      "key": "quality",
      "name": "Data Quality",
      "icon": "\u2705",
      "weight": 1.3,
      "description": "Profiling, monitoring, business rules, and remediation across critical data elements.",
      "next_steps": [
        "Profile critical data elements and define data quality dimensions and thresholds.",
        "Automate rule checks and alerting; integrate remediation workflows.",
        "Publish quality scorecards to business owners with trend analysis."
      ],
      "sub_dimensions": [
        {
          "key": "profiling",
          "name": "Profiling & Monitoring",
          "levels": {
            "1": "No routine profiling; unknown data quality.",
            "2": "Periodic manual profiling on select systems.",
            "3": "Automated profiling for key domains with dashboards.",
            "4": "Continuous monitoring with alerts and SLA responses.",
            "5": "Predictive quality with prevention embedded in pipelines."
          }
        },
        {
          "key": "rules",
          "name": "Business Rules & Thresholds",
          "levels": {
            "1": "No documented rules or thresholds.",
            "2": "Some rules defined by teams; inconsistent.",
            "3": "Enterprise rule library with owners and change control.",
            "4": "Rules versioned, tested, and measured for impact.",
            "5": "Rules auto-generated from metadata/lineage; continuous learning."
          }
        },
        {
          "key": "remediation",
          "name": "Remediation Workflow",
          "levels": {
            "1": "Manual fixes; no systematic approach.",
            "2": "Email-driven fixes; scattered ownership.",
            "3": "Ticketed workflow with owners and SLAs.",
            "4": "Root-cause standardization; playbooks for common issues.",
            "5": "Automated remediation for recurring patterns; human-in-the-loop."
          }
        }
      ]
    },
    {
      "key": "metadata",
      "name": "Metadata & Catalog",
      "icon": "\ud83d\uddc2\ufe0f",
      "weight": 1.1,
      "description": "Business glossary, technical lineage, and catalog practices to enable discoverability and trust.",
      "next_steps": [
        "Stand up a data catalog; seed with priority systems and glossary terms.",
        "Define lineage for critical pipelines; connect glossary \u2194 technical assets.",
        "Enable social features (ratings, certifications) to drive adoption."
      ],
      "sub_dimensions": [
        {
          "key": "glossary",
          "name": "Business Glossary",
          "levels": {
            "1": "No glossary; terms ambiguous across teams.",
            "2": "Team-specific glossaries; inconsistent definitions.",
            "3": "Enterprise glossary with owners and review cadence.",
            "4": "Glossary integrated with BI and catalog; change control in place.",
            "5": "Glossary linked to policies, controls, and KPIs; usage analytics."
          }
        },
        {
          "key": "lineage",
          "name": "Technical Lineage",
          "levels": {
            "1": "Unknown lineage; tribal knowledge.",
            "2": "Manual lineage diagrams for select pipelines.",
            "3": "Tool-supported lineage for priority datasets.",
            "4": "End-to-end lineage integrated into CI/CD and impact analysis.",
            "5": "Automated lineage with quality gates and drift detection."
          }
        },
        {
          "key": "cataloging",
          "name": "Cataloging Practices",
          "levels": {
            "1": "No central catalog or inventory.",
            "2": "Partial inventory; not actively maintained.",
            "3": "Central catalog with curation responsibilities.",
            "4": "Certification, usage telemetry, and deprecation lifecycle.",
            "5": "Active curation with recommendations and semantic enrichment."
          }
        }
      ]
    },
    {
      "key": "architecture",
      "name": "Architecture & Integration",
      "icon": "\ud83c\udfd7\ufe0f",
      "weight": 1.0,
      "description": "Integration patterns, platform reliability, and interoperability across the data estate.",
      "next_steps": [
        "Standardize integration patterns (batch, CDC, streaming) with reference designs.",
        "Harden environments with IaC, observability, and cost governance.",
        "Adopt domain-oriented data products with clear SLAs and APIs."
      ],
      "sub_dimensions": [
        {
          "key": "integration",
          "name": "Integration Methods",
          "levels": {
            "1": "Ad hoc file drops and manual imports.",
            "2": "Point-to-point integrations; fragile dependencies.",
            "3": "Standardized patterns (ETL/ELT/CDC) for core systems.",
            "4": "Streaming/CDC and orchestration; reusable connectors.",
            "5": "Self-serve data products and federated gateways."
          }
        },
        {
          "key": "platform",
          "name": "Platform Reliability & Ops",
          "levels": {
            "1": "Unreliable pipelines; outages common.",
            "2": "Basic monitoring; manual recoveries.",
            "3": "Observability and alerting; RTO/RPO targets",
            "4": "IaC, blue/green, cost guardrails; routine DR tests.",
            "5": "SLOs with error budgets; automated scaling and resilience."
          }
        },
        {
          "key": "interoperability",
          "name": "Interoperability",
          "levels": {
            "1": "Closed systems; custom adapters everywhere.",
            "2": "Some APIs; inconsistent standards.",
            "3": "Adopts open standards (FHIR, HL7, DICOM) in key domains.",
            "4": "Enterprise-level contract testing and schema governance.",
            "5": "Semantic interoperability and knowledge graph layer."
          }
        }
      ]
    },
    {
      "key": "security",
      "name": "Security & Privacy",
      "icon": "\ud83d\udd10",
      "weight": 1.2,
      "description": "Access controls, privacy engineering, and compliance-aligned data protection.",
      "next_steps": [
        "Implement role-based access and least privilege with periodic reviews.",
        "Data classification and masking for sensitive attributes; audit logging.",
        "Automate privacy impact assessments and consent management."
      ],
      "sub_dimensions": [
        {
          "key": "access",
          "name": "Access Control",
          "levels": {
            "1": "Shared credentials; over-privileged access.",
            "2": "Role-based access for some systems.",
            "3": "Central IAM with periodic access reviews.",
            "4": "Attribute-based access; fine-grained policies in pipelines.",
            "5": "Just-in-time access with continuous verification."
          }
        },
        {
          "key": "privacy",
          "name": "Privacy Engineering",
          "levels": {
            "1": "No systematic handling of PII/PHI.",
            "2": "Manual redaction/masking as needed.",
            "3": "Standardized masking/tokenization for sensitive data.",
            "4": "Differential privacy/k-anonymity considered for analytics.",
            "5": "Privacy by design with automated policy enforcement."
          }
        },
        {
          "key": "compliance",
          "name": "Compliance Alignment",
          "levels": {
            "1": "Unclear regulatory obligations.",
            "2": "Reactive compliance fixes post-audit.",
            "3": "Mapped controls to regulations (HIPAA, GDPR, etc.).",
            "4": "Continuous control monitoring and evidence collection.",
            "5": "Integrated GRC with automated attestations."
          }
        }
      ]
    },
    {
      "key": "analytics",
      "name": "Analytics & AI Readiness",
      "icon": "\ud83e\udd16",
      "weight": 1.0,
      "description": "Business intelligence adoption, ML/AI foundations, and responsible AI practices.",
      "next_steps": [
        "Harden data marts and semantic layers for governed BI self-service.",
        "Establish ML ops foundations (feature store, experiment tracking, model registry).",
        "Adopt responsible AI guidelines and bias/robustness evaluation."
      ],
      "sub_dimensions": [
        {
          "key": "bi_adoption",
          "name": "BI Adoption",
          "levels": {
            "1": "Reports built manually; spreadsheets dominate.",
            "2": "Central BI exists; low adoption.",
            "3": "Departmental self-service with governed datasets.",
            "4": "Enterprise metrics store; usage telemetry drives improvements.",
            "5": "Decision automation with business-owned analytics products."
          }
        },
        {
          "key": "ml_foundations",
          "name": "ML/AI Foundations",
          "levels": {
            "1": "Ad hoc notebooks; no formal lifecycle.",
            "2": "Isolated pilots; limited reproducibility.",
            "3": "Versioned data/models; basic MLOps in place.",
            "4": "Feature store, CI/CD for ML, monitoring of drift/perf.",
            "5": "Responsible AI by default; continuous training and governance."
          }
        },
        {
          "key": "ethics",
          "name": "Responsible AI",
          "levels": {
            "1": "No policy for AI ethics/bias.",
            "2": "Awareness but no formal practices.",
            "3": "Documented guidelines and review board.",
            "4": "Bias/robustness tests integrated in lifecycle.",
            "5": "Auditable AI with human-in-the-loop and red-teaming."
          }
        }
      ]
    }
  ]
}